{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf2556f2beda18f1f26971d6efd7076c6651a973",
    "heading_collapsed": true
   },
   "source": [
    "# INF554 Kaggle Challenge\n",
    "\n",
    "By Ombeline Lagé & Haris Sahovic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package importations & data location\n",
    "\n",
    "Some of these elements are not used anymore, as they were used for code that has been removed or modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "1a54bedb77b3307af0b484565846c4f269af12f6"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3be87b3019cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mheapq\u001b[0m  \u001b[0;31m# Used for fast top-k element retrieval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m###########################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/chunk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkParserI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m from nltk.chunk.util import (ChunkScore, accuracy, tagstr2tree, conllstr2tree,\n\u001b[1;32m    159\u001b[0m                              \u001b[0mconlltags2tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree2conllstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/chunk/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m##//////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParserI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/parse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmalt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaltParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDependencyEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransitionparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransitionParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbllip\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBllipParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorenlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoreNLPParser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCoreNLPDependencyParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/parse/transitionparser.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_svmlight_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/svm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# License: BSD 3 clause (C) INRIA 2010\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNuSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNuSVR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneClassSVM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mLinearSVR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbounds\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0ml1_min_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/svm/classes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_fit_liblinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseLibSVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRegressorMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutlierMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearClassifierMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparseCoefMixin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mLinearModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBayesianRidge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mARDRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m from .least_angle import (Lars, LassoLars, lars_path, LarsCV, LassoLarsCV,\n\u001b[0m\u001b[1;32m     16\u001b[0m                           LassoLarsIC)\n\u001b[1;32m     17\u001b[0m from .coordinate_descent import (Lasso, ElasticNet, LassoCV, ElasticNetCV,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegressorMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marrayfuncs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_float_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvergenceWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeprecationDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_is_arraylike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetaestimators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0m_BaseComposition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_metaclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mABCMeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \"\"\"Handles parameter management for classifiers composed of named estimators.\n\u001b[1;32m     20\u001b[0m     \"\"\"\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/abc.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(mcls, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_abc_registry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWeakSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_abc_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWeakSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_abc_negative_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWeakSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_abc_negative_cache_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mABCMeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_abc_invalidation_counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/_weakrefset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0m_remove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselfref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselfref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import heapq  # Used for fast top-k element retrieval\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "import re\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "from IPython.display import SVG, display  # User for Keras model-graph display\n",
    "from operator import add\n",
    "\n",
    "from keras.backend import set_value\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import concatenate as concatenate_layers\n",
    "from keras.layers import AlphaDropout, Conv1D, Conv2D, Dense, Dropout, Embedding, Flatten, GaussianNoise, GlobalAveragePooling1D, GlobalMaxPool1D, GRU, Input, LSTM, MaxPooling1D\n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.optimizers import Adadelta, Adagrad, Nadam, RMSprop, SGD\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data location\n",
    "\n",
    "If running on your computer, please place the following files in a folder named `data`:\n",
    "\n",
    "- `testing_set.txt`\n",
    "- `training_set.txt`\n",
    "- `node_information.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7400f7adb572dd9b91ad513c6801619977af2a4"
   },
   "outputs": [],
   "source": [
    "if True:  # Kaggle notebook\n",
    "    to_predict_location = \"../input/testing_set.txt\"\n",
    "    training_location = \"../input/training_set.txt\"\n",
    "    node_location = \"../input/node_information.csv\"\n",
    "if True:\n",
    "    to_predict_location = os.path.join(\"data\", \"testing_set.txt\")\n",
    "    training_location = os.path.join(\"data\", \"training_set.txt\")\n",
    "    node_location = os.path.join(\"data\", \"node_information.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d886cb56904cebf877c2242949f594783e113ec8"
   },
   "source": [
    "## Nodes importation\n",
    "\n",
    "Importation of raw node data.\n",
    "\n",
    "Nodes are stored in a `list` named `nodes`.\n",
    "\n",
    "Each node is a `dict` instance, with entries as follows :\n",
    "- `id`: node id (`int`)\n",
    "- `year`: article year of publication (`int`)\n",
    "- `title`: article title (`str`)\n",
    "- `authors_raw`: article authors, unprocessed (`str`)\n",
    "- `journal_raw`: article journal, unprocessed (`str`)\n",
    "- `abstract`: article abstract, unprocessed (`str`)\n",
    "\n",
    "We also create `nodes_dict`, a dictionnary allowing access to node by their `id`, ie. given a node `n`, we have `nodes_dict[n['id']] == n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "565a16b6ffdea1f21656a42c1a8e1bc2c143e895",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nodes = []\n",
    "\n",
    "with open(node_location) as f:\n",
    "    data_reader = csv.reader(f)\n",
    "    for line in data_reader:\n",
    "        node = {\n",
    "            'id': int(line[0]),\n",
    "            'year': int(line[1]),\n",
    "            'title': line[2],\n",
    "            'authors_raw': line[3],\n",
    "            'journal_raw': line[4],\n",
    "            'abstract': line[5],\n",
    "        }\n",
    "\n",
    "        nodes.append(node)\n",
    "\n",
    "nodes_dict = {\n",
    "    n['id']: n for n in nodes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7005d159ece31976ea82dfbd7b9ffeb6e3bc679b"
   },
   "source": [
    "## Training and to submit data importation\n",
    "\n",
    "The data is imported in two lists, `training_input` and `to_predict_input`. Each list contains lists with two `int`s, the `id`s of two nodes.\n",
    "\n",
    "Training output is also imported in `training_output`, a `list` of `int`s (actually, a list of `1`s and `0`s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff1ff5cbeddae3b25c1660d7c106f33e7d7f8b23"
   },
   "outputs": [],
   "source": [
    "training_input = []\n",
    "training_output = []\n",
    "\n",
    "to_predict_input = []\n",
    "\n",
    "with open(training_location) as f:\n",
    "    for line in f.read().split('\\n'):\n",
    "        line = [int(el) for el in line.split(' ')]\n",
    "        training_input.append(line[:2])\n",
    "        training_output.append(line[-1])\n",
    "\n",
    "with open(to_predict_location) as f:\n",
    "    for line in f.read().split('\\n'):\n",
    "        line = [int(el) for el in line.split(' ')]\n",
    "        to_predict_input.append(line[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "Here is a quick reference to the content of each `node` after feature extraction:\n",
    "- Raw features\n",
    "- `id` : node id (`int`)\n",
    "    - `year`: article's year of publication (`int`)\n",
    "    - `title`: article's title (`str`)\n",
    "    - `authors_raw`: article's authors, unprocessed (`str`)\n",
    "    - `journal_raw`: article's journal, unprocessed (`str`)\n",
    "    - `abstract`: article's abstract, unprocessed (`str`)\n",
    "- Journal & authors features\n",
    "    - `authors`: `list` of authors (`str`) of the article\n",
    "    - `journal`: article's journal, processed (`str`)\n",
    "    - `authors_int`: `list` of `int`s representing the article authors (see `author_to_int`)\n",
    "    - `journal_int`: `int` representing the article's journal (see `journal_to_int`)\n",
    "    - `authors_one_hot`: one-hot vector (`np.array`) of article's authors\n",
    "    - `journal_ont_hot`: one-hot vectror (`np.array`) of article's journal\n",
    "- NLP features\n",
    "    - `abstract_tfidf`: tfidf vector of the journal's abstract\n",
    "    - `title_tfidf`: tfidf vector of the journal's title\n",
    "    - `abstract_svd`: svd of the journal's abstract\n",
    "    - `title_svd`: svd of the journal's title\n",
    "    - `abstract_int`: index (`list` of `int`s) representation of the journal's abstract\n",
    "    - `title_int`: index (`list` of `int`s) representation of the journal's title\n",
    "    - `authors_svd`: svd of the journal's authors titles\n",
    "    - `abstract_wv`: array (`np.array`) representing word2vec vectors of the journal's abstract\n",
    "    - `title_wv`: array (`np.array`) representing word2vec vectors of the journal's title\n",
    "- Graph features\n",
    "    - `co_citation`: `dict` containing the number of co-citation with a given nodes\n",
    "    - see `graph_features` for the rest of them\n",
    "\n",
    "\n",
    "\n",
    "Other useful objects, used for conversion from `str` to index (`int`) representation.\n",
    "\n",
    "- `author_to_int`: a `dict` associating an `int`to each author represented by a `str`\n",
    "- `int_to_author`: a `list` such that `author_to_int[int_to_author[i]] == i`\n",
    "- `journal_to_int`: a `dict` associating an `int`to each jounral represented by a `str`\n",
    "- `int_to_journal`: a `list` such that `journal_to_int[int_to_journal[i]] == i`\n",
    "\n",
    "Other things :\n",
    "- `author_titles`:  `dict` mapping each author `int` to a list of its article titles\n",
    "- `words_list`: sorted list of gensim tokens\n",
    "- `word_to_int`: reverse index of `words_list`\n",
    "- `embeddings`: matrix using w2v or Glove vectors for Keras embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author & journal formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journal & Author indexing\n",
    "\n",
    "Here, we:\n",
    "\n",
    "- Extract the list of authors for each node, clean the authors, and add the list as `authors` (`list` of `str`)\n",
    "- Convert authors to `int`s. We can convert authors to `int`s and `int`s to authors with `author_to_int` (`dict`) and `int_to_author` (`list`)\n",
    "- Add a `list` of `int`s representing its authors to each node, in an `authors_int` entry.\n",
    "- Do some text formatting on journals and save the result as `journal` for each node\n",
    "- Convert journals to `int`s. We can convert journals to `int`s and `int`s to journals with `journal_to_int` (`dict`) and `int_to_journal` (`list`)\n",
    "- Store the node journal `int` into `journal_int`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_to_int = {}\n",
    "int_to_author = []\n",
    "\n",
    "journal_to_int = {}\n",
    "int_to_journal = []\n",
    "\n",
    "\n",
    "def clean_author(author: str) -> str:\n",
    "    author = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", author)\n",
    "    if ')' in author:\n",
    "        author = author[author.index(')')+1:]\n",
    "    if '(' in author:\n",
    "        author = author[:author.index('(')]\n",
    "    while author.startswith(' '):\n",
    "        author = author[1:]\n",
    "    while author.endswith(' '):\n",
    "        author = author[:-1]\n",
    "    author = author.replace('-', ' ')\n",
    "    author = author.replace(' ', '')\n",
    "    return author\n",
    "\n",
    "\n",
    "for n in nodes:\n",
    "    n['authors'] = [clean_author(aut)\n",
    "                    for aut in re.split(\"&|,|;\", n['authors_raw'])]\n",
    "    for author in n['authors']:\n",
    "        if not author:\n",
    "            continue\n",
    "        if author not in author_to_int:\n",
    "            author_to_int[author] = len(author_to_int)\n",
    "            int_to_author.append(author)\n",
    "    n['authors_int'] = [author_to_int[author]\n",
    "                        for author in n['authors'] if author]\n",
    "    n['journal'] = n['journal_raw'].replace(\"'\", '').replace('\"', '').replace(\"\\\\\", '').replace(\n",
    "        \"/\", '').replace(\")\", '').replace(\"(\", '').replace(\"-\", '').replace(\",\", '').lower()\n",
    "    if n['journal'] not in journal_to_int:\n",
    "        journal_to_int[n['journal']] = len(journal_to_int)\n",
    "        int_to_journal.append(n['journal'])\n",
    "    n['journal_int'] = journal_to_int[n['journal']]\n",
    "\n",
    "\n",
    "nodes_dict = {\n",
    "    n['id']: n for n in nodes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author title\n",
    "\n",
    "We create `author_titles`, a `dict` mapping each author `int` to a list of its article titles. This can be used to infer topics by authors, later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_titles = {}\n",
    "\n",
    "for n in nodes:\n",
    "    for author_int in n['authors_int']:\n",
    "        if author_int not in author_titles:\n",
    "            author_titles[author_int] = [n['title']]\n",
    "        else:\n",
    "            author_titles[author_int].append(n['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f268852829bb8211615de941d097f49a8a18c84e"
   },
   "source": [
    "### One-hot author vectors\n",
    "\n",
    "One-hot vector of article authors, stored as `authors_one_hot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "26a6f255ed181cf46c82f0064d945637f3aef470"
   },
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    auth_vector = np.zeros(len(author_to_int))\n",
    "    for auth in node['authors_int']:\n",
    "        auth_vector[auth] = 1\n",
    "    node['authors_one_hot'] = auth_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d008e0d87959898e2b96a9873e0321dee159dc68"
   },
   "source": [
    "### One-hot journal vectors\n",
    "\n",
    "One-hot vector of article journal, stored as `journal_one_hot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a56b5cb282c03ab17ebdd1f564e4d3b804995030"
   },
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    journal_vector = np.zeros(len(journal_to_int))\n",
    "    journal_vector[journal_to_int[node['journal']]] = 1\n",
    "    node['journal_one_hot'] = journal_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d09e2873bdc763bc2e1e23379372108ae102a3d5"
   },
   "source": [
    "### Authors PCA\n",
    "\n",
    "PCA on one-hot vector of authors, with dimension `AUTHOR_PCA_DIM`, stored in `authors_pca`.\n",
    "\n",
    "The low value of the explained variance indicates that this PCA does not represents very adequatly our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d594f109ed0a90458fe06347b6b2361dacc49094"
   },
   "outputs": [],
   "source": [
    "AUTHOR_PCA_DIM = 24\n",
    "\n",
    "author_pca = PCA(AUTHOR_PCA_DIM)\n",
    "\n",
    "for n, pca in zip(nodes, author_pca.fit_transform([n['authors_one_hot'] for n in nodes])):\n",
    "    n['authors_pca'] = pca\n",
    "\n",
    "print('Authors dimension:', len(n['authors_one_hot']))\n",
    "print(\"Explained variance: %.3f\" % (sum(author_pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journal PCA\n",
    "\n",
    "PCA on one-hot vector of journals, with dimension `JOURNAL_PCA_DIM`, stored in `journal_pca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOURNAL_PCA_DIM = 24\n",
    "\n",
    "journal_pca = PCA(JOURNAL_PCA_DIM)\n",
    "for node, pca in zip(nodes, journal_pca.fit_transform([n['journal_one_hot'] for n in nodes])):\n",
    "    node['journal_pca'] = pca\n",
    "\n",
    "print('Journal dimension:', len(n['journal_one_hot']))\n",
    "print(\"Explained variance: %.3f\" %\n",
    "      (sum(journal_pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "We apply gensim's preprocessing to nodes titles and abstracts. The results are stored in `title_gensim` and `abstract_gensim`. We also build a list of words, `words_list` on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = set()  # Initialised as a set for quick updating\n",
    "\n",
    "for n in nodes:\n",
    "    n['title_gensim'] = [w for w in gensim.utils.simple_preprocess(n['title'], max_len=30)]\n",
    "    n['abstract_gensim'] = [w for w in gensim.utils.simple_preprocess(n['abstract'], max_len=30)]\n",
    "    words_list.update(set(n['title_gensim']).union(n['abstract_gensim']))\n",
    "\n",
    "# Converted and sorted for reproducible results\n",
    "words_list = sorted(list(words_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3e829d3e7287e02706fd2478d6ca51774d9f89b"
   },
   "source": [
    "### Naive approach\n",
    "\n",
    "First approach. We define a `naive_similarity` function, which returns, given two nodes:\n",
    "- The percentage of words of a's abstract in b's abstract\n",
    "- The percentage of words of a's abstract in b's title\n",
    "- The percentage of words of a's title in b's abstract\n",
    "- The percentage of words of a's title in b's title\n",
    "- The percentage of words of b's abstract in a's abstract\n",
    "- The percentage of words of b's abstract in a's title\n",
    "- The percentage of words of b's title in a's abstract\n",
    "- The percentage of words of b's title in a's title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d945f6467716bcf49f164946aba4812713228357"
   },
   "outputs": [],
   "source": [
    "def naive_similarity(node_a, node_b):\n",
    "    sta = set(node_a['title_gensim'])\n",
    "    saa = set(node_a['abstract_gensim'])\n",
    "    sab = set(node_b['abstract_gensim'])\n",
    "\n",
    "    tt = len((sta).intersection(node_b['title_gensim']))\n",
    "    ta = len((sab).intersection(sta))\n",
    "    at = len((saa).intersection(node_b['title_gensim']))\n",
    "    aa = len((saa).intersection(sab))\n",
    "\n",
    "    lta = len(node_a['title_gensim'])\n",
    "    ltb = len(node_b['title_gensim'])\n",
    "    laa = len(node_a['abstract_gensim'])\n",
    "    lab = len(node_b['abstract_gensim'])\n",
    "\n",
    "    return (tt / lta if tt else 0,\n",
    "            tt / ltb if tt else 0,\n",
    "            ta / lta if ta else 0,\n",
    "            ta / lab if ta else 0,\n",
    "            at / laa if at else 0,\n",
    "            at / ltb if at else 0,\n",
    "            aa / laa if aa else 0,\n",
    "            aa / lab if aa else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF and LSA\n",
    "\n",
    "We use `scikit-learn`'s `TfidfVectorizer` to compute tfidf vectors of nodes abstracts and titles. We use a `ngram_range` of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF\n",
    "\n",
    "To each node, we add respectively:\n",
    "- `abstract_tfidf`: tfidf vector of the journal's abstract\n",
    "- `title_tfidf`: tfidf vector of the journal's title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(\n",
    "    [n['abstract'] for n in nodes] + [n['title'] for n in nodes]\n",
    ")\n",
    "\n",
    "for n, a_tfidf, t_tfidf in zip(nodes, tfidf_vectors[:len(nodes)], tfidf_vectors[len(nodes):]):\n",
    "    n['abstract_tfidf'] = a_tfidf\n",
    "    n['title_tfidf'] = t_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD\n",
    "\n",
    "Now that we have tfidf vectors, we apply a truncated SVD to get LSA vectors of dimension `SVD_DIM`. The resulting vectors are added to the nodes as:\n",
    "\n",
    "- `abstract_svd`\n",
    "- `title_svd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD_DIM = 128\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=SVD_DIM)\n",
    "svd_matrix = svd_model.fit_transform(tfidf_vectors)\n",
    "\n",
    "for n, vec in zip(nodes, svd_matrix[:len(nodes)]):\n",
    "    n['abstract_svd'] = vec\n",
    "for n, vec in zip(nodes, svd_matrix[len(nodes):]):\n",
    "    n['title_svd'] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tfidf_vectors, svd_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors titles' SVD\n",
    "\n",
    "For each node, we group it's authors articles titles' and apply our LSA. The result is added as `authors_svd`.\n",
    "\n",
    "For each author, we group the titles of their articles\n",
    "\n",
    "Même procédé pour chaque auteur, où l'on agrège titres et abstracts par auteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for node, vec in zip(nodes, svd_model.transform(\n",
    "        tfidf_vectorizer.transform([\n",
    "            '\\n'.join([\n",
    "                title\n",
    "                for aut in n['authors_int']\n",
    "                for title in author_titles[aut]\n",
    "            ])\n",
    "            for n in nodes\n",
    "        ])\n",
    "    )):\n",
    "        node['authors_svd'] = vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common words SVD\n",
    "\n",
    "Given two nodes, `common_words_svd` computes the LSA representation of their common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_words_svd(node_a, node_b):\n",
    "    a = set(node_a['abstract_gensim']).union(node_a['title_gensim'])\n",
    "    b = set(node_b['abstract_gensim']).union(node_b['title_gensim'])\n",
    "    return svd_model.transform(\n",
    "        tfidf_vectorizer.transform([' '.join(a.intersection(b))])\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training / Loading\n",
    "\n",
    "Training or loading of the w2v model.\n",
    "If `TRAIN_W2V` is set to `True`, the model will be trained on the titles / abstract corpus, and produce vectors of dimension `W2V_VEC_DIM`. Furthermore, they will be saved in `trained_w2v.model`.\n",
    "\n",
    "Otherwise, a model will be loaded. If `LOAD_GOOGLE` is `True`, Google's embeddings are going to be used. If not, the last trained model will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_W2V = True\n",
    "LOAD_GOOGLE = False\n",
    "W2V_FILE = \"trained_w2v.model\"\n",
    "W2V_VEC_DIM = 128\n",
    "\n",
    "if TRAIN_W2V:\n",
    "    w2v_training_set = [t\n",
    "                        for n in nodes\n",
    "                        for t in [n['title_gensim'], n['abstract_gensim']]\n",
    "                        ]\n",
    "    w2v_model = gensim.models.Word2Vec(\n",
    "        w2v_training_set,\n",
    "        size=W2V_VEC_DIM,\n",
    "        window=7,\n",
    "        min_count=0,\n",
    "        workers=120\n",
    "    )\n",
    "    w2v_model.train(\n",
    "        w2v_training_set,\n",
    "        total_examples=len(w2v_training_set),\n",
    "        epochs=120\n",
    "    )\n",
    "    w2v_model.wv.save_word2vec_format(W2V_FILE)\n",
    "    del w2v_training_set\n",
    "else:\n",
    "    if LOAD_GOOGLE:\n",
    "        W2V_VEC_DIM = 300\n",
    "        w2v_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            '/Users/ombeline/Desktop/GoogleNews-vectors-negative300.bin', # Here, the location of google's w2v vectors\n",
    "            binary=True\n",
    "        )\n",
    "    else:  # Last trained model\n",
    "        w2v_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            W2V_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node w2v vectors\n",
    "\n",
    "Now that we have a word2vec model to work with, we can use it to compute word2vec representation of our textual features, namely titles and abstracts.\n",
    "\n",
    "In order to do this, we need first to compute the maximum length of our vectors, so that they can have a fixed size.\n",
    "\n",
    "Max sizes are stored in `LEN_TITLE` and `LEN_ABSTRACT`; node word2vec vectors are stored into `title_wv` and `abstract_wv` arrays for each node.\n",
    "\n",
    "Storing w2v vectors for each node is expensive, memory wise. If this feature is not used, its storage can be turned off by setting `USE_W2V_NODES` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_W2V_NODES = False\n",
    "\n",
    "LEN_TITLE = max([\n",
    "    len((n['title_gensim']))\n",
    "    for n in nodes\n",
    "])\n",
    "LEN_ABSTRACT = max([\n",
    "    len((n['abstract_gensim']))\n",
    "    for n in nodes\n",
    "])\n",
    "\n",
    "if USE_W2V_NODES:\n",
    "    for n in nodes: # Switch to keras' pad_sequence ?\n",
    "        n['title_wv'] = np.zeros((LEN_TITLE, W2V_VEC_DIM))\n",
    "        n['abstract_wv'] = np.zeros((LEN_ABSTRACT, W2V_VEC_DIM))\n",
    "\n",
    "        for i, w in enumerate(n['title_gensim']):\n",
    "            n['title_wv'][i] = w2v_model[w] if w in w2v_model else np.zeros(\n",
    "                VEC_DIM)\n",
    "        for i, w in enumerate(n['abstract_gensim']):\n",
    "            n['abstract_wv'][i] = w2v_model[w] if w in w2v_model else np.zeros(\n",
    "                VEC_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embeddings\n",
    "\n",
    "We also define embeddings, to be used by Keras.\n",
    "\n",
    "We use `words_list` to build an `embeddings` matrix, and inject our word2vec representation as vectors. We keep `0` as a special value for empty entries. We also build a `word_to_int` dictionnary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_int = {w: i+1 for i, w in enumerate(words_list)}\n",
    "embeddings = np.zeros((len(word_to_int) + 1, W2V_VEC_DIM))\n",
    "\n",
    "for w, i in word_to_int.items():\n",
    "    if w in w2v_model:\n",
    "        embeddings[i] = w2v_model[w]\n",
    "    else:\n",
    "        embeddings[i] = np.random.normal(W2V_VEC_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lists of word indexes\n",
    "\n",
    "To use this embedding, we need corresponding representations of our sentences, as `list`s of index `int`s. They are stored as `title_int` and `abstract_int`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in nodes:\n",
    "    n['title_int'] = pad_sequences([[word_to_int[w] for w in n['title_gensim']]], maxlen=LEN_TITLE)[0]\n",
    "    n['abstract_int'] = pad_sequences([[word_to_int[w] for w in n['abstract_gensim']]], maxlen=LEN_ABSTRACT)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common words w2v\n",
    "\n",
    "A first application of w2v can be to retain the common words of two phrases, and compute their w2v representation.\n",
    "\n",
    "That is the goal of the function `w2v_common_words`. The function:\n",
    "- selects common words\n",
    "- if there are less than 5, we add the word 'the' as much as necessay, effectively acting as a 0 padding\n",
    "- if there is more than 5, we only keep the 5 rarest words\n",
    "- return value is the concatenation of the w2v vectors of the remaining words\n",
    "\n",
    "For efficiency, we precompute `tfidf_voc` and `tfidf_dic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_voc = set(tfidf_vectorizer.vocabulary_.keys())\n",
    "tfidf_dic = {w: i for w,i in tfidf_vectorizer.vocabulary_.items()}\n",
    "lookup = list(tfidf_vectorizer.idf_)\n",
    "tfidf_dic = {w: lookup[i] for w,i in tfidf_dic.items()}\n",
    "\n",
    "def w2v_common_words(node_a, node_b, n_words=5):\n",
    "    common_words = set(\n",
    "        node_a['abstract_gensim'] + node_a['title_gensim']\n",
    "    ).intersection(\n",
    "        node_b['title_gensim'] + node_b['abstract_gensim']\n",
    "    ).intersection(tfidf_voc)\n",
    "\n",
    "    if len(common_words) < n_words:\n",
    "        common_words = list(common_words) + \\\n",
    "            (['the'] * (n_words - len(common_words)))\n",
    "    else:\n",
    "        common_words = heapq.nlargest(\n",
    "            n_words, common_words, key=lambda w: tfidf_dic[w])\n",
    "    return [el for w in common_words for el in w2v_model[w]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove embedding\n",
    "\n",
    "Alternative work embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GLOVE_EMBEDDING = False\n",
    "\n",
    "if USE_GLOVE_EMBEDDING:\n",
    "    location = \"glove.6B.300d.txt\" # to be changed to your location\n",
    "    glove_embedding = {}\n",
    "    with open(location) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            glove_embedding[word] = coefs\n",
    "    W2V_VEC_DIM = 300\n",
    "    embeddings = np.zeros((len(word_to_int) + 1, W2V_VEC_DIM))\n",
    "    for w, i in word_to_int.items():\n",
    "        if w in glove_embedding:\n",
    "            embeddings[i] = glove_embedding[w]\n",
    "        else:\n",
    "            embeddings[i] = np.zeros(W2V_VEC_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dc0fda0c67d2f36a314b6806e4b140995675bb7c"
   },
   "source": [
    "### Author cooperation graph\n",
    "\n",
    "We build a graph of author cooperation, and build a `class` to compute distances based on this distance. Two authors that have worked together have a distance of `0`; two authors that have a common co-author have a distance of `1`, and so on.\n",
    "\n",
    "We then instanciate this `AuthorCooperationDistance` class in `author_distance`, with our graph. This instance is callable as a function, and does what you expect it to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20c9f257c5de664499a390ce77ea2158d53ba811"
   },
   "outputs": [],
   "source": [
    "author_neighbours = {a: set() for a in author_to_int.values()}\n",
    "for node in nodes:\n",
    "    for aut in node['authors_int']:\n",
    "        author_neighbours[aut].update(node['authors_int'])\n",
    "\n",
    "\n",
    "class AuthorCooperationDistance():\n",
    "    \"\"\"\n",
    "    This class is used as a lazy distance matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, neighbours: List[int]) -> None:\n",
    "        self.n = neighbours\n",
    "        self.d = {}  # Computed distances\n",
    "\n",
    "    def __call__(self, a: int, b: int) -> int:\n",
    "        if a in self.d:\n",
    "            pass\n",
    "        elif b in self.d:\n",
    "            if a in self.d[b]:\n",
    "                if a not in self.d:\n",
    "                    self.d[a] = {}\n",
    "                self.d[a][b] = self.d[b][a]\n",
    "            else:\n",
    "                if a not in self.d:\n",
    "                    self.d[a] = {}\n",
    "                self.d[a][b] = self.d[b][a] = None\n",
    "        else:\n",
    "            self.compute_distance(a)\n",
    "        if b not in self.d[a]:\n",
    "            self.d[a][b] = None\n",
    "        return self.d[a][b]\n",
    "\n",
    "    def compute_distance(self, a: int) -> None:\n",
    "        \"\"\"Computes distances of authors linked to a.\"\"\"\n",
    "        if a not in self.d:\n",
    "            self.d[a] = {}\n",
    "\n",
    "        to_explore = self.n[a]\n",
    "        explored = set()\n",
    "        i = 0\n",
    "\n",
    "        while to_explore:\n",
    "            to_add = set()\n",
    "            for element in to_explore:\n",
    "                if element in explored:\n",
    "                    continue\n",
    "                self.d[a][element] = i\n",
    "                to_add.update(self.n[element])\n",
    "            explored.update(to_explore)\n",
    "            to_explore = to_add\n",
    "            i += 1\n",
    "\n",
    "\n",
    "author_distance = AuthorCooperationDistance(author_neighbours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ff992df79989b62a932015d5fd96c24e6426530"
   },
   "source": [
    "### Distance of author groups\n",
    "\n",
    "We are looking for a featuring measuring the distance between two groups of authors. We start by computing the cooperation distances pairwise, group to group. We then extract five features:\n",
    "\n",
    "- First group's size\n",
    "- Second group's size\n",
    "- Mean of distances with a value\n",
    "- Min distance\n",
    "- The proportion of unconnected pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "864c8b5a3225ede698b00bbf9466d1b50c0af5c9"
   },
   "outputs": [],
   "source": [
    "def author_set_comparison(aut_a, aut_b):\n",
    "    n_none = 0\n",
    "    mean = 0\n",
    "    min_d = None\n",
    "    for a in aut_a:\n",
    "        for b in aut_b:\n",
    "            d = (author_distance(a, b))\n",
    "            if d is None:\n",
    "                n_none += 1\n",
    "            else:\n",
    "                mean += d\n",
    "                if min_d is None or min_d > d:\n",
    "                    min_d = d\n",
    "    return np.array([\n",
    "        len(aut_a),\n",
    "        len(aut_b),\n",
    "        mean / max((len(aut_a) * len(aut_b)) - n_none, 1),\n",
    "        n_none/max((len(aut_a) * len(aut_b)),1),\n",
    "        min_d if min_d else 5\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes arity\n",
    "\n",
    "We compute, for each journal and authors, the number of incoming and outcoming citations, and build a citation graph.\n",
    "\n",
    "Furthermore, for each article, we compute the number of incoming citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_citations = {}\n",
    "a_citations = {}\n",
    "\n",
    "j_citations = {}\n",
    "j_out = {}\n",
    "a_out = {}\n",
    "\n",
    "j_in = {}\n",
    "a_in = {}\n",
    "n_in = {}\n",
    "\n",
    "max_a_citations = 0\n",
    "min_a_citations = 64\n",
    "\n",
    "cites = {n['id']: set() for n in nodes} # ->\n",
    "\n",
    "id_to_int = {n['id']:i for i, n in enumerate(nodes)}\n",
    "\n",
    "for inp, out in zip(training_input, training_output):\n",
    "    if out == 1:\n",
    "        node_a = nodes_dict[inp[0]]\n",
    "        node_b = nodes_dict[inp[1]]\n",
    "        n_in[inp[1]] = n_in.get(inp[1], -1) + 1\n",
    "\n",
    "        cites[inp[0]].add(inp[1])        \n",
    "        \n",
    "        aut_a = node_a['authors_int']\n",
    "        aut_b = node_b['authors_int']\n",
    "        j_a = journal_to_int[node_a['journal']]\n",
    "        j_b = journal_to_int[node_b['journal']]\n",
    "\n",
    "        max_a_citations = max(max_a_citations, len(aut_a) * len(aut_b))\n",
    "        min_a_citations = min(min_a_citations, len(aut_a) * len(aut_b))\n",
    "\n",
    "        if j_a not in j_citations:\n",
    "            j_citations[j_a] = {}\n",
    "\n",
    "        if j_b not in j_citations[j_a]:\n",
    "            j_citations[j_a][j_b] = 0\n",
    "        else:\n",
    "            j_citations[j_a][j_b] += 1\n",
    "\n",
    "        j_out[j_a] = j_out.get(j_a, -1) + 1\n",
    "        j_in[j_b] = j_in.get(j_b, -1) + 1\n",
    "        for a in aut_a:\n",
    "            for b in aut_b:\n",
    "                if a not in a_citations:\n",
    "                    a_citations[a] = {}\n",
    "                if b not in a_citations[a]:\n",
    "                    a_citations[a][b] = 0\n",
    "                else:\n",
    "                    a_citations[a][b] += 1\n",
    "                a_out[a] = a_out.get(a, -1) + 1\n",
    "                a_in[b] = a_in.get(b, -1) + 1\n",
    "\n",
    "j_distance = np.zeros((len(int_to_journal), len(int_to_journal)))\n",
    "for i in range(len(int_to_journal)):\n",
    "    if i not in j_citations:\n",
    "        continue\n",
    "    for j, val in j_citations[i].items():\n",
    "        j_distance[i][j] = val/j_out[i] if j_out[i] else 0\n",
    "\n",
    "a_distance = np.zeros((len(author_to_int), len(author_to_int)))\n",
    "for i in range(len(author_to_int)):\n",
    "    if i not in a_citations:\n",
    "        continue\n",
    "    for j, val in a_citations[i].items():\n",
    "        a_distance[i][j] = val/a_out[i] if a_out[i] else 0\n",
    "\n",
    "for n in nodes:\n",
    "    n['popularity'] = n_in.get(n['id'], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjacency matrices\n",
    "\n",
    "We build direct, reversed and undirected adjency matrices for articles, authors and journals.\n",
    "\n",
    "Then, we compute their squares, cube and hypercube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_cites = [(aa, ab) \n",
    "           for a, set_b in cites.items() \n",
    "           for b in set_b \n",
    "           for aa in nodes_dict[a]['authors_int'] \n",
    "           for ab in nodes_dict[b]['authors_int']\n",
    "          ]\n",
    "j_cites = [(nodes_dict[a]['journal_int'], nodes_dict[b]['journal_int']) \n",
    "           for a, set_b in cites.items() \n",
    "           for b in set_b \n",
    "          ]\n",
    "n_cites = [(id_to_int[a], id_to_int[b]) for a, set_b in cites.items() for b in set_b]\n",
    "\n",
    "node_d_adjacence = scipy.sparse.coo_matrix(\n",
    "    ([1 for _ in n_cites], \n",
    "     ([a for a, b in n_cites], \n",
    "      [b for a, b in n_cites])\n",
    "    ), \n",
    "    shape = (len(nodes), len(nodes))\n",
    ").tocsr()\n",
    "\n",
    "node_r_adjacence = scipy.sparse.coo_matrix(\n",
    "    ([1 for _ in n_cites], \n",
    "     ([b for a, b in n_cites], \n",
    "      [a for a, b in n_cites])\n",
    "    ), \n",
    "    shape = (len(nodes), len(nodes))\n",
    ").tocsr()\n",
    "\n",
    "node_u_adjacence = scipy.sparse.coo_matrix(\n",
    "    ([1 for _ in n_cites] + [1 for _ in n_cites], \n",
    "     ([a for a, b in n_cites] + [b for a, b in n_cites], \n",
    "      [b for a, b in n_cites] + [a for a, b in n_cites])\n",
    "    ), \n",
    "    shape = (len(nodes), len(nodes))\n",
    ").tocsr()\n",
    "\n",
    "author_d_adjacence = scipy.sparse.coo_matrix(\n",
    "    ([1 for _ in a_cites], \n",
    "     ([a for a, b in a_cites], \n",
    "      [b for a, b in a_cites])\n",
    "    ), \n",
    "    shape = (len(int_to_author), len(int_to_author))\n",
    ").tocsr()\n",
    "\n",
    "author_r_adjacence = scipy.sparse.coo_matrix(\n",
    "    ([1 for _ in a_cites], \n",
    "     ([b for a, b in a_cites], \n",
    "      [a for a, b in a_cites])\n",
    "    ), \n",
    "    shape = (len(int_to_author), len(int_to_author))\n",
    ").tocsr()\n",
    "\n",
    "author_u_adjacence = scipy.sparse.coo_matrix(\n",
    "    ([1 for _ in a_cites] + [1 for _ in a_cites], \n",
    "     ([a for a, b in a_cites] + [b for a, b in a_cites], \n",
    "      [b for a, b in a_cites] + [a for a, b in a_cites])\n",
    "    ), \n",
    "    shape = (len(int_to_author), len(int_to_author))\n",
    ").tocsr()\n",
    "\n",
    "journal_d_adjacence = scipy.sparse.coo_matrix(\n",
    "    ([1 for _ in j_cites], \n",
    "     ([a for a, b in j_cites], \n",
    "      [b for a, b in j_cites])\n",
    "    ), \n",
    "    shape = (len(int_to_journal), len(int_to_journal))\n",
    ").tocsr()\n",
    "\n",
    "journal_r_adjacence = scipy.sparse.coo_matrix(\n",
    "    ([1 for _ in j_cites], \n",
    "     ([b for a, b in j_cites], \n",
    "      [a for a, b in j_cites])\n",
    "    ), \n",
    "    shape = (len(int_to_journal), len(int_to_journal))\n",
    ").tocsr()\n",
    "\n",
    "journal_u_adjacence = scipy.sparse.coo_matrix(\n",
    "    ([1 for _ in j_cites] + [1 for _ in j_cites], \n",
    "     ([a for a, b in j_cites] + [b for a, b in j_cites], \n",
    "      [b for a, b in j_cites] + [a for a, b in j_cites])\n",
    "    ), \n",
    "    shape = (len(int_to_journal), len(int_to_journal))\n",
    ").tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nodes direct adjacence matrix of order 2')\n",
    "node_d_adjacence_2 = node_d_adjacence ** 2\n",
    "print('Nodes direct adjacence matrix of order 3')\n",
    "node_d_adjacence_3 = node_d_adjacence_2 * node_d_adjacence_2\n",
    "print('Nodes direct adjacence matrix of order 4')\n",
    "node_d_adjacence_4 = node_d_adjacence_3 * node_d_adjacence_2\n",
    "print('Nodes reversed adjacence matrix of order 2')\n",
    "node_r_adjacence_2 = node_r_adjacence ** 2\n",
    "print('Nodes reversed adjacence matrix of order 3')\n",
    "node_r_adjacence_3 = node_r_adjacence_2 * node_r_adjacence_2\n",
    "print('Nodes reversed adjacence matrix of order 4')\n",
    "node_r_adjacence_4 = node_r_adjacence_3 * node_r_adjacence_2\n",
    "print('Nodes undirected adjacence matrix of order 2')\n",
    "node_u_adjacence_2 = node_u_adjacence ** 2\n",
    "print('Nodes undirected adjacence matrix of order 3')\n",
    "node_u_adjacence_3 = node_u_adjacence_2 * node_u_adjacence_2\n",
    "print('Nodes undirected adjacence matrix of order 4')\n",
    "node_u_adjacence_4 = node_u_adjacence_3 * node_d_adjacence_2\n",
    "print('Author direct adjacence matrix of order 2')\n",
    "author_d_adjacence_2 = author_d_adjacence ** 2\n",
    "print('Author direct adjacence matrix of order 2')\n",
    "author_r_adjacence_2 = author_u_adjacence ** 2\n",
    "print('Author undirected adjacence matrix of order 2')\n",
    "author_u_adjacence_2 = author_u_adjacence ** 2\n",
    "print('Author direct adjacence matrix of order 3')\n",
    "author_d_adjacence_3 = author_d_adjacence_2 * author_d_adjacence\n",
    "print('Author direct adjacence matrix of order 3')\n",
    "author_r_adjacence_3 = author_u_adjacence_2  * author_r_adjacence\n",
    "print('Author undirected adjacence matrix of order 3')\n",
    "author_u_adjacence_3 = author_u_adjacence_2 * author_u_adjacence\n",
    "print('Author direct adjacence matrix of order 4')\n",
    "author_d_adjacence_4 = author_d_adjacence_3 * author_d_adjacence\n",
    "print('Author direct adjacence matrix of order 4')\n",
    "author_r_adjacence_4 = author_u_adjacence_3  * author_r_adjacence\n",
    "print('Author undirected adjacence matrix of order 4')\n",
    "author_u_adjacence_4 = author_u_adjacence_3  * author_u_adjacence\n",
    "print('Journal direct adjacence matrix of order 2')\n",
    "journal_d_adjacence_2 = journal_d_adjacence ** 2\n",
    "print('Journal reversed adjacence matrix of order 2')\n",
    "journal_r_adjacence_2 = journal_u_adjacence ** 2\n",
    "print('Journal undirected adjacence matrix of order 2')\n",
    "journal_u_adjacence_2 = journal_u_adjacence ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-citation\n",
    "\n",
    "We say two nodes are co-cited if they are cited by the same node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in nodes:\n",
    "    n['co_citation'] = dict()\n",
    "\n",
    "for cited in cites.values():\n",
    "    for a in cited:\n",
    "        for b in cited:\n",
    "            nodes_dict[a]['co_citation'][b] = nodes_dict[a]['co_citation'].get(b, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node vectorisation\n",
    "\n",
    "We try to apply lsa directly on nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_VEC_DIM = 128\n",
    "print()\n",
    "node_vectorizer = TfidfVectorizer()\n",
    "node_tfidf = node_vectorizer.fit_transform(\n",
    "    [' '.join([str(u) for u in v]) for v in cites.values()]\n",
    ")\n",
    "\n",
    "node_svd = TruncatedSVD(n_components=NODE_VEC_DIM).fit(node_tfidf)\n",
    "\n",
    "def vectorize_nodes(node_a, node_b):\n",
    "    ida = node_a['id']\n",
    "    idb = node_b['id']\n",
    "    sa = cites[ida]\n",
    "    sb = cites[idb]\n",
    "\n",
    "    return node_svd.transform(\n",
    "            node_vectorizer.transform(\n",
    "                [' '.join([str(u) for u in v if u not in [ida, idb]]) for v in [sa, sb]]\n",
    "            )\n",
    "        ).reshape(2 * NODE_VEC_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph features function\n",
    "\n",
    "The information we extracted above is not directly linked to a node; we therefore define a function returning relevant features. Given two nodes `node_a` and `node_b`, the returned features are:\n",
    "\n",
    "- Boolean indicating if `node_a` has a journal\n",
    "- Boolean indicating if `node_a` has a journal\n",
    "- Distance going from `journal_a` to `journal_b`\n",
    "- Distance going from `journal_b` to `journal_a`\n",
    "- Minimum 4 citation distance from `node_a`'s authors to `node_b`'s\n",
    "- Minimum 4 citation distance from `node_b`'s authors to `node_a`'s\n",
    "- Number of citation from `node_a`'s journal\n",
    "- Number of citation to `node_a`'s journal\n",
    "- Number of citation to `node_a`\n",
    "- Number of citation from `node_b`'s journal\n",
    "- Number of citation to `node_b`'s journal\n",
    "- Number of citation to `node_b`\n",
    "- Adjaceny matrices entries for our two nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_features(node_a, node_b):\n",
    "    return [\n",
    "        1 if node_a['journal'] != '' else 0,\n",
    "        1 if node_b['journal'] != '' else 0,\n",
    "        node_a['co_citation'].get(node_b['id'], 0),\n",
    "        j_distance[journal_to_int[node_a['journal']]\n",
    "                   ][journal_to_int[node_b['journal']]],\n",
    "        j_distance[journal_to_int[node_b['journal']]\n",
    "                   ][journal_to_int[node_a['journal']]],\n",
    "        j_out.get(node_a['journal'], 0),\n",
    "        j_in.get(node_a['journal'], 0),\n",
    "        n_in.get(node_a['journal'], 0),\n",
    "        j_out.get(node_b['journal'], 0),\n",
    "        j_in.get(node_b['journal'], 0),\n",
    "        n_in.get(node_b['journal'], 0),\n",
    "        node_d_adjacence_2[id_to_int[node_a['id']], id_to_int[node_b['id']]],\n",
    "        node_d_adjacence_3[id_to_int[node_a['id']], id_to_int[node_b['id']]],\n",
    "        node_d_adjacence_4[id_to_int[node_a['id']], id_to_int[node_b['id']]],\n",
    "        node_r_adjacence_2[id_to_int[node_a['id']], id_to_int[node_b['id']]],\n",
    "        node_r_adjacence_3[id_to_int[node_a['id']], id_to_int[node_b['id']]],\n",
    "        node_r_adjacence_4[id_to_int[node_a['id']], id_to_int[node_b['id']]],\n",
    "        node_u_adjacence_2[id_to_int[node_a['id']], id_to_int[node_b['id']]],\n",
    "        node_u_adjacence_3[id_to_int[node_a['id']], id_to_int[node_b['id']]],\n",
    "        node_u_adjacence_4[id_to_int[node_a['id']], id_to_int[node_b['id']]],\n",
    "        sum([author_d_adjacence_2[a, b] for a in node_a['authors_int'] for b in node_b['authors_int']]),\n",
    "        sum([author_r_adjacence_2[a, b] for a in node_a['authors_int'] for b in node_b['authors_int']]),\n",
    "        sum([author_u_adjacence_2[a, b] for a in node_a['authors_int'] for b in node_b['authors_int']]),\n",
    "        sum([author_d_adjacence_3[a, b] for a in node_a['authors_int'] for b in node_b['authors_int']]),\n",
    "        sum([author_r_adjacence_3[a, b] for a in node_a['authors_int'] for b in node_b['authors_int']]),\n",
    "        sum([author_u_adjacence_3[a, b] for a in node_a['authors_int'] for b in node_b['authors_int']]),\n",
    "        sum([author_d_adjacence_4[a, b] for a in node_a['authors_int'] for b in node_b['authors_int']]),\n",
    "        sum([author_r_adjacence_4[a, b] for a in node_a['authors_int'] for b in node_b['authors_int']]),\n",
    "        sum([author_u_adjacence_4[a, b] for a in node_a['authors_int'] for b in node_b['authors_int']]),\n",
    "        journal_d_adjacence_2[node_a['journal_int'], node_b['journal_int']],\n",
    "        journal_r_adjacence_2[node_a['journal_int'], node_b['journal_int']],\n",
    "        journal_u_adjacence_2[node_a['journal_int'], node_b['journal_int']],\n",
    "        node_d_adjacence_2[id_to_int[node_b['id']], id_to_int[node_a['id']]],\n",
    "        node_d_adjacence_3[id_to_int[node_b['id']], id_to_int[node_a['id']]],\n",
    "        node_d_adjacence_4[id_to_int[node_b['id']], id_to_int[node_a['id']]],\n",
    "        node_r_adjacence_2[id_to_int[node_b['id']], id_to_int[node_a['id']]],\n",
    "        node_r_adjacence_3[id_to_int[node_b['id']], id_to_int[node_a['id']]],\n",
    "        node_r_adjacence_4[id_to_int[node_b['id']], id_to_int[node_a['id']]],\n",
    "        sum([author_d_adjacence_2[b, a] for a in node_b['authors_int'] for b in node_a['authors_int']]),\n",
    "        sum([author_r_adjacence_2[b, a] for a in node_b['authors_int'] for b in node_a['authors_int']]),\n",
    "        sum([author_d_adjacence_3[b, a] for a in node_a['authors_int'] for b in node_b['authors_int']]),\n",
    "        sum([author_r_adjacence_3[b, a] for a in node_a['authors_int'] for b in node_b['authors_int']]),\n",
    "        sum([author_d_adjacence_4[b, a] for a in node_a['authors_int'] for b in node_b['authors_int']]),\n",
    "        sum([author_r_adjacence_4[b, a] for a in node_a['authors_int'] for b in node_b['authors_int']]),\n",
    "        journal_d_adjacence_2[node_b['journal_int'], node_a['journal_int']],\n",
    "        journal_r_adjacence_2[node_b['journal_int'], node_a['journal_int']],\n",
    "    ] + heapq.nsmallest(4,\n",
    "                        [a_distance[i][j]\n",
    "                         for i in node_a['authors_int']\n",
    "                         for j in node_b['authors_int']\n",
    "                         ] + [0, 0, 0, 0]\n",
    "                        )\\\n",
    "    + heapq.nsmallest(4,\n",
    "                        [a_distance[j][i]\n",
    "                         for i in node_a['authors_int']\n",
    "                         for j in node_b['authors_int']\n",
    "                         ] + [0, 0, 0, 0]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c594a2cce5bf2ed94a5645a01b5e72c8e88391a"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "Three functions:\n",
    "\n",
    "- `entry`: takes two nodes as an argument, returns layer-ready features\n",
    "- `inputify`: creates a keras-ready matrix with the selected features\n",
    "- `thresholdify`: given predictions and values, compute an approximate of the optimal threshold for the f1_score\n",
    "\n",
    "We also set our `TRAINING_TEST_RATIO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d49a1e0c07d27a3bbd73154bda86a0bdb6e7a932",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "TRAINING_TEST_RATIO = .9\n",
    "\n",
    "def entry(node_a, node_b):\n",
    "    return np.concatenate([\n",
    "        [node_a['year'],\n",
    "         node_b['year'],\n",
    "         np.dot(node_a['title_svd'],node_b['title_svd']),\n",
    "         np.dot(node_a['abstract_svd'],node_b['title_svd']),\n",
    "         np.dot(node_a['title_svd'],node_b['abstract_svd']),\n",
    "         np.dot(node_a['abstract_svd'],node_b['abstract_svd']),\n",
    "         (node_a['title_tfidf'] * node_b['title_tfidf'].T).A[0][0],\n",
    "         (node_a['abstract_tfidf'] * node_b['abstract_tfidf'].T).A[0][0],\n",
    "         (node_a['title_tfidf'] * node_b['abstract_tfidf'].T).A[0][0],\n",
    "         (node_a['abstract_tfidf'] * node_b['title_tfidf'].T).A[0][0]],\n",
    "        graph_features(node_a, node_b),\n",
    "        node_a['journal_pca'],\n",
    "        node_b['journal_pca'],\n",
    "        author_set_comparison(node_a['authors_int'], node_b['authors_int']),\n",
    "        naive_similarity(node_a, node_b),\n",
    "        node_a['title_svd'],\n",
    "        node_b['title_svd'],\n",
    "        node_a['abstract_svd'],\n",
    "        node_b['abstract_svd'],\n",
    "        vectorize_nodes(node_a, node_b)]\n",
    "    )\n",
    "\n",
    "def inputify(source: List[List]):\n",
    "    start = time.time()\n",
    "    features = np.empty((len(source), len(\n",
    "        list(entry(nodes_dict[source[0][0]], nodes_dict[source[0][0]])))))\n",
    "    print(':%s:\\n:' % ('.' * 101), end='')\n",
    "    for i, el in enumerate(source):\n",
    "        if i % int(len(source) / 100) == 0:\n",
    "            print('.', end='')\n",
    "        node_a = nodes_dict[el[0]]\n",
    "        node_b = nodes_dict[el[1]]\n",
    "        features[i] = entry(node_a, node_b)\n",
    "    print(':\\nFinished inputifying in %f secondes.' % (time.time() - start))\n",
    "    return features\n",
    "\n",
    "def thresholdify(predictions, y, a: float = 0.01, b: float = .99, partitions=8, depth=4) -> float:\n",
    "    for _ in range(depth):\n",
    "        t = a\n",
    "        best_f = 0\n",
    "        best_t = None\n",
    "        epsilon = (b-a) / partitions\n",
    "\n",
    "        while t <= b:\n",
    "            f = f1_score([int(el + t) for el in predictions], y)\n",
    "            if f > best_f:\n",
    "                best_f = f\n",
    "                best_t = t\n",
    "            t += epsilon\n",
    "        a = a if best_t == a else best_t - epsilon\n",
    "        b = b if best_t == b else best_t + epsilon\n",
    "    return (a+b) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common features definition\n",
    "\n",
    "If every feature is used, this can be long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_FEATURES = True\n",
    "SAVE_FEATURES = True\n",
    "\n",
    "if COMPUTE_FEATURES:\n",
    "    normaliser = StandardScaler()\n",
    "    (training_set,\n",
    "     testing_set,\n",
    "     training_set_output,\n",
    "     testing_set_output,\n",
    "     training_indexes,\n",
    "     testing_indexes) = train_test_split(\n",
    "        normaliser.fit_transform(\n",
    "            inputify(training_input)\n",
    "        ),\n",
    "        training_output,\n",
    "        training_input,\n",
    "        train_size=TRAINING_TEST_RATIO\n",
    "    )\n",
    "    \n",
    "    output_set = normaliser.transform(inputify(to_predict_input))\n",
    "    if SAVE_FEATURES:\n",
    "        np.save('os.np', output_set)\n",
    "        np.save('trs.np',training_set)\n",
    "        np.save('tes.np',testing_set)\n",
    "        np.save('tso.np',training_set_output)\n",
    "        np.save('teo.np',testing_set_output)\n",
    "        np.save('tri.np',training_indexes)\n",
    "        np.save('tei.np',testing_indexes)\n",
    "else:\n",
    "    training_set = np.load('trs.np.npy')\n",
    "    testing_set = np.load('tes.np.npy')\n",
    "    training_set_output = np.load('tso.np.npy')\n",
    "    testing_set_output = np.load('teo.np.npy')\n",
    "    training_indexes = np.load('tri.np.npy')\n",
    "    testing_indexes = np.load('tei.np.npy')\n",
    "    output_set = np.load('os.np.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn predictions\n",
    "\n",
    "We create sklearn classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "SIZE = len(training_indexes)\n",
    "\n",
    "t = time.time()\n",
    "l_svc = LinearSVC()\n",
    "l_svc.fit(training_set[:SIZE], training_set_output[:SIZE])\n",
    "print(\"Linear SVC accuracy: %f in %fs\" % (l_svc.score(testing_set, testing_set_output), time.time() - t))\n",
    "\n",
    "t = time.time()\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(training_set[:SIZE], training_set_output[:SIZE])\n",
    "print(\"SGD accuracy: %f in %fs\" % (sgd.score(testing_set, testing_set_output), time.time() - t))\n",
    "\n",
    "t = time.time()\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(training_set[:SIZE], training_set_output[:SIZE])\n",
    "print(\"Random forest accuracy: %f in %fs\" % (forest.score(testing_set, testing_set_output), time.time() - t))\n",
    "\n",
    "t = time.time()\n",
    "boost = AdaBoostClassifier()\n",
    "boost.fit(training_set[:SIZE], training_set_output[:SIZE])\n",
    "print(\"AdaBoost accuracy: %f in %fs\" % (boost.score(testing_set, testing_set_output), time.time() - t))\n",
    "\n",
    "\n",
    "t = time.time()\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(training_set[:SIZE], training_set_output[:SIZE])\n",
    "print(\"Gaussian NB accuracy: %f in %fs\" % (gnb.score(testing_set, testing_set_output), time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "forest_2 = RandomForestClassifier(30)\n",
    "forest_2.fit(training_set[:SIZE], training_set_output[:SIZE])\n",
    "print(\"Random forest accuracy: %f in %fs\" % (forest_2.score(testing_set, testing_set_output), time.time() - t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "forest_3 = RandomForestClassifier(100)\n",
    "forest_3.fit(training_set[:SIZE], training_set_output[:SIZE])\n",
    "print(\"Random forest accuracy: %f in %fs\" % (forest_3.score(testing_set, testing_set_output), time.time() - t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = forest_3.predict_proba(testing_set)\n",
    "t = thresholdify(pred[:,1], testing_set_output)\n",
    "pred = [int(t+el[1]) for el in pred]\n",
    "f1_score(pred, testing_set_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "forest_4 = RandomForestClassifier(400)\n",
    "forest_4.fit(training_set[:SIZE], training_set_output[:SIZE])\n",
    "print(\"Random forest accuracy: %f in %fs\" % (forest_4.score(testing_set, testing_set_output), time.time() - t))\n",
    "\n",
    "pred = forest_4.predict_proba(testing_set)\n",
    "t = thresholdify(pred[:,1], testing_set_output)\n",
    "pred = [int(t+el[1]) for el in pred]\n",
    "f1_score(pred, testing_set_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_pred = [int(t+el[1]) for el in forest_3.predict_proba(output_set)]\n",
    "\n",
    "with open(\"out.csv\", \"w+\") as f:\n",
    "    f.write(\"id,category\\n\")\n",
    "    for i, v in enumerate([int(el + t) for el in o_pred]):\n",
    "        f.write(\"%d,%d\\n\" % (i, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Composite Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_AUTHORS = max([len(n['authors_int']) for n in nodes])\n",
    "\n",
    "def lm_content_generator(source, other_features, output, infinite=True):\n",
    "    while True:\n",
    "        for el, f, o in zip(source, other_features, output):\n",
    "            node_a, node_b = nodes_dict[el[0]], nodes_dict[el[1]]\n",
    "            yield (node_a['authors_int'],\n",
    "                   node_b['authors_int'],\n",
    "                   node_a['journal_int'],\n",
    "                   node_b['journal_int'],\n",
    "                   f,\n",
    "                   o\n",
    "                   )\n",
    "        if not infinite:\n",
    "            break\n",
    "\n",
    "\n",
    "def lm_batchify(source, other_features, output, batch_size):\n",
    "    generator = lm_content_generator(source, other_features, output)\n",
    "    try:\n",
    "        while True:\n",
    "            inp = {'authors_1': np.full((batch_size, LEN_AUTHORS), len(author_to_int)),\n",
    "                   'authors_2': np.full((batch_size, LEN_AUTHORS), len(author_to_int)),\n",
    "                   'journal_1': np.empty((batch_size,)),\n",
    "                   'journal_2': np.empty((batch_size,)),\n",
    "                   'other_features': np.empty((batch_size, len(other_features[0]))),\n",
    "                   }\n",
    "            out = np.empty((batch_size))\n",
    "            for i in range(batch_size):\n",
    "                el = next(generator)\n",
    "                inp['authors_1'][i][:len(el[0])] = el[0]\n",
    "                inp['authors_2'][i][:len(el[1])] = el[1]\n",
    "                inp['journal_1'][i] = el[2]\n",
    "                inp['journal_2'][i] = el[3]\n",
    "                inp['other_features'][i] = el[4]\n",
    "                out[i] = el[5]\n",
    "            yield inp, (out)\n",
    "    except StopIteration:\n",
    "        yield inp, (out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9737c78777e868eef48037243976fceab9d0f9c9"
   },
   "outputs": [],
   "source": [
    "NOISE = 0\n",
    "DROPOUT = .4\n",
    "JOURNAL_DIM = 16\n",
    "AUTHORS_DIM = 6\n",
    "\n",
    "authors_1 = Input(shape=(LEN_AUTHORS,), name='authors_1')\n",
    "authors_2 = Input(shape=(LEN_AUTHORS,), name='authors_2')\n",
    "\n",
    "journal_1 = Input(shape=(1,), name='journal_1')  # Abstract 1\n",
    "journal_2 = Input(shape=(1,), name='journal_2')  # Abstract 2\n",
    "\n",
    "embedding_authors = Embedding(input_dim=len(author_to_int) + 1,\n",
    "                        output_dim=AUTHORS_DIM,\n",
    "                        input_length=LEN_AUTHORS,\n",
    "                        trainable=True,\n",
    "                        )\n",
    "\n",
    "embedding_journal = Embedding(input_dim=len(journal_to_int),\n",
    "                        output_dim=JOURNAL_DIM,\n",
    "                        input_length=1,\n",
    "                        trainable=True,\n",
    "                        )\n",
    "\n",
    "journal_1_output = Flatten()(embedding_journal(journal_1))\n",
    "journal_2_output = Flatten()(embedding_journal(journal_2))\n",
    "\n",
    "authors_1_output = LSTM(AUTHORS_DIM)(embedding_authors(authors_1))\n",
    "authors_2_output = LSTM(AUTHORS_DIM)(embedding_authors(authors_2))\n",
    "\n",
    "other_features = Input(shape=(len(training_set[0]),), name='other_features')\n",
    "\n",
    "dense_input = concatenate_layers([authors_1_output, authors_2_output, journal_1_output, journal_2_output, other_features])\n",
    "dense_1 = Dropout(DROPOUT)(Dense(256, activation='elu')(dense_input))\n",
    "dense_2 = Dropout(DROPOUT)(Dense(128, activation='elu')(dense_1))\n",
    "output = Dense(1, activation='sigmoid', name=\"output\")(dense_2)\n",
    "\n",
    "model = Model(inputs=[authors_1, authors_2, journal_1, journal_2, other_features], outputs=(output))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "display(SVG(model_to_dot(model).create(prog='dot', format='svg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 320\n",
    "N_EPOCHS = 40\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc',\n",
    "                               patience=3,\n",
    "                               mode='max',\n",
    "                               verbose=1)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"dense-{val_acc:.4f}.hdf5\",\n",
    "                               monitor='val_acc',\n",
    "                               save_best_only=True,\n",
    "                               verbose=1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc',\n",
    "                              factor=0.33,\n",
    "                              patience=0,\n",
    "                              cooldown=1,\n",
    "                              verbose=True)\n",
    "\n",
    "model.fit_generator(\n",
    "    lm_batchify(training_indexes, training_set,\n",
    "                training_set_output, BATCH_SIZE),\n",
    "    epochs=N_EPOCHS,\n",
    "    steps_per_epoch=int(len(training_set)/BATCH_SIZE)+1,\n",
    "    validation_data=lm_batchify(\n",
    "        testing_indexes, testing_set, testing_set_output, BATCH_SIZE),\n",
    "    validation_steps=int(len(testing_set)/BATCH_SIZE)+1,\n",
    "    callbacks=[checkpointer, reduce_lr, early_stopping],\n",
    "    use_multiprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = thresholdify(\n",
    "        model.predict_generator(\n",
    "            lm_batchify(\n",
    "                testing_indexes, \n",
    "                testing_set, \n",
    "                testing_set_output, \n",
    "                len(testing_indexes)\n",
    "            ), steps = 1), \n",
    "        testing_set_output\n",
    "    )\n",
    "\n",
    "augmented_testing_set = []\n",
    "\n",
    "augmented_output_set = []\n",
    "\n",
    "for el in zip(\n",
    "    #boost.predict_proba(testing_set),\n",
    "    #forest.predict_proba(testing_set),\n",
    "    #forest_2.predict_proba(testing_set),\n",
    "    forest_3.predict_proba(testing_set),\n",
    "    forest_4.predict_proba(testing_set),\n",
    "    #l_svc.predict(testing_set),\n",
    "    #sgd.predict_proba(testing_set),\n",
    "    #gnb.predict(testing_set),\n",
    "    [el for el in model.predict_generator(\n",
    "        lm_batchify(\n",
    "           testing_indexes, \n",
    "           testing_set, [0 for _ in testing_indexes], \n",
    "           len(testing_indexes)\n",
    "       ), \n",
    "   steps = 1\n",
    "    )]):\n",
    "    augmented_testing_set.append(sum([l[-1] for l in el])/len(el))\n",
    "        \n",
    "t = thresholdify(augmented_testing_set, testing_set_output)\n",
    "print(f1_score([int(el + t) for el in augmented_testing_set], testing_set_output))\n",
    "\n",
    "for el in zip(\n",
    "    #boost.predict_proba(output_set),\n",
    "    #forest.predict_proba(output_set),\n",
    "    # forest_2.predict_proba(output_set),\n",
    "    forest_3.predict_proba(output_set),\n",
    "    forest_4.predict_proba(output_set),\n",
    "    #l_svc.predict(output_set),\n",
    "    #sgd.predict(output_set),\n",
    "    #gnb.predict_proba(output_set),\n",
    "    [el for el in model.predict_generator(\n",
    "        lm_batchify(\n",
    "           to_predict_input, \n",
    "           output_set, [0 for _ in output_set], \n",
    "           len(to_predict_input)\n",
    "       ), \n",
    "   steps = 1\n",
    "    )]\n",
    "                            ):\n",
    "    augmented_output_set.append(sum([l[-1] for l in el])/len(el))\n",
    "\n",
    "\n",
    "with open(\"out.csv\", \"w+\") as f:\n",
    "    f.write(\"id,category\\n\")\n",
    "    for i, v in enumerate([int(el + t) for el in augmented_output_set]):\n",
    "        f.write(\"%d,%d\\n\" % (i, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model - output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "augmented_testing_set = []\n",
    "augmented_output_set = []\n",
    "\n",
    "for el in zip(\n",
    "    #boost.predict_proba(testing_set),\n",
    "    #forest.predict_proba(testing_set),\n",
    "    #forest_2.predict_proba(testing_set),\n",
    "    forest_3.predict_proba(testing_set),\n",
    "    forest_4.predict_proba(testing_set),\n",
    "    #l_svc.predict(testing_set),\n",
    "    #sgd.predict_proba(testing_set),\n",
    "    #gnb.predict(testing_set),\n",
    "    [el for el in model.predict_generator(\n",
    "        lm_batchify(\n",
    "           testing_indexes, \n",
    "           testing_set, [0 for _ in testing_indexes], \n",
    "           len(testing_indexes)\n",
    "       ), \n",
    "   steps = 1\n",
    "    )]):\n",
    "    augmented_testing_set.append(sum([l[-1] for l in el])/len(el))\n",
    "\n",
    "t = thresholdify(augmented_testing_set, testing_set_output)\n",
    "print(f1_score([int(el+t) for el in augmented_testing_set], testing_set_output))\n",
    "\n",
    "for el in zip(\n",
    "    #boost.predict_proba(output_set),\n",
    "                        #forest.predict_proba(output_set),\n",
    "                         # forest_2.predict_proba(output_set),\n",
    "   forest_3.predict_proba(output_set),\n",
    "    forest_4.predict_proba(output_set),\n",
    "                        #l_svc.predict(output_set),\n",
    "                        #sgd.predict(output_set),\n",
    "                        #gnb.predict_proba(output_set),\n",
    "                        [el for el in model.predict_generator(\n",
    "                            lm_batchify(\n",
    "                               to_predict_input, \n",
    "                               output_set, [0 for _ in output_set], \n",
    "                               len(to_predict_input)\n",
    "                           ), \n",
    "                       steps = 1\n",
    "                        )]\n",
    "                            ):\n",
    "    augmented_output_set.append(sum([l[-1] for l in el])/len(el))\n",
    "\n",
    "with open(\"out.csv\", \"w+\") as f:\n",
    "    f.write(\"id,category\\n\")\n",
    "    for i, v in enumerate([int(el + t) for el in augmented_output_set]):\n",
    "        f.write(\"%d,%d\\n\" % (i, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "This section is not to be used for the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e8e7a62a29a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# stopping notebook run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# stopping notebook run\n",
    "assert 1 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "\n",
    "Two functions:\n",
    "\n",
    "- `content_generator`: yields inputs and outputs, line by line\n",
    "- `batchify`: keras-ready generator creating batches using `content_generator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_AUTHORS = max([len(n['authors_int']) for n in nodes])\n",
    "\n",
    "def cnn_content_generator(source, other_features, output, infinite=True):\n",
    "    while True:\n",
    "        for el, f, o in zip(source, other_features, output):\n",
    "            node_a, node_b = nodes_dict[el[0]], nodes_dict[el[1]]\n",
    "            yield (node_a['authors_int'],\n",
    "                   node_b['authors_int'],\n",
    "                   node_a['journal_int'],\n",
    "                   node_b['journal_int'],\n",
    "                   node_a['title_int'],\n",
    "                   node_b['title_int'],\n",
    "                   node_a['abstract_int'],\n",
    "                   node_b['abstract_int'],\n",
    "                   f,\n",
    "                   o\n",
    "                   )\n",
    "        if not infinite:\n",
    "            break\n",
    "\n",
    "\n",
    "def cnn_batchify(source, other_features, output, batch_size):\n",
    "    generator = cnn_content_generator(source, other_features, output)\n",
    "    try:\n",
    "        while True:\n",
    "            inp = {'authors_1': np.full((batch_size, LEN_AUTHORS), len(author_to_int)),\n",
    "                   'authors_2': np.full((batch_size, LEN_AUTHORS), len(author_to_int)),\n",
    "                   'journal_1': np.empty((batch_size,)),\n",
    "                   'journal_2': np.empty((batch_size,)),\n",
    "                   'titles_1': np.empty((batch_size, LEN_TITLE)),\n",
    "                   'titles_2': np.empty((batch_size, LEN_TITLE)),\n",
    "                   'abstracts_1': np.empty((batch_size, LEN_ABSTRACT)),\n",
    "                   'abstracts_2': np.empty((batch_size, LEN_ABSTRACT)),\n",
    "                   'other_features': np.empty((batch_size, len(training_set[0]))),\n",
    "                   }\n",
    "            out = np.empty((batch_size))\n",
    "            for i in range(batch_size):\n",
    "                el = next(generator)\n",
    "                inp['authors_1'][i][:len(el[0])] = el[0]\n",
    "                inp['authors_2'][i][:len(el[1])] = el[1]\n",
    "                inp['journal_1'][i] = el[2]\n",
    "                inp['journal_2'][i] = el[3]\n",
    "                inp['titles_1'][i] = el[4]\n",
    "                inp['titles_2'][i] = el[5]\n",
    "                inp['abstracts_1'][i] = el[6]\n",
    "                inp['abstracts_2'][i] = el[7]\n",
    "                inp['other_features'][i] = el[8]\n",
    "                out[i] = el[9]\n",
    "            yield inp, {'output':out, 'cnn_output':out}\n",
    "    except StopIteration:\n",
    "        yield inp, {out, out}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_FEATURES = 96\n",
    "A_FEATURES = 96\n",
    "DROPOUT = 0.25\n",
    "TRAIN_EMBEDDING = False\n",
    "JOURNAL_DIM = 8\n",
    "AUTHORS_DIM = 6\n",
    "\n",
    "authors_1 = Input(shape=(LEN_AUTHORS,), name='authors_1')\n",
    "authors_2 = Input(shape=(LEN_AUTHORS,), name='authors_2')\n",
    "\n",
    "journal_1 = Input(shape=(1,), name='journal_1')  # Abstract 1\n",
    "journal_2 = Input(shape=(1,), name='journal_2')  # Abstract 2\n",
    "\n",
    "title_1 = Input(shape=(LEN_TITLE,), name='titles_1')\n",
    "title_2 = Input(shape=(LEN_TITLE,), name='titles_2')\n",
    "\n",
    "abstract_1 = Input(shape=(LEN_ABSTRACT,), name='abstracts_1')  # Abstract 1\n",
    "abstract_2 = Input(shape=(LEN_ABSTRACT,), name='abstracts_2')  # Abstract 2\n",
    "\n",
    "other_features = Input(shape=(len(training_set[0]),), name='other_features')\n",
    "\n",
    "embedding_authors = Embedding(input_dim=len(author_to_int) + 1,\n",
    "                        output_dim=AUTHORS_DIM,\n",
    "                        input_length=LEN_AUTHORS,\n",
    "                        trainable=TRAIN_EMBEDDING,\n",
    "                        )\n",
    "\n",
    "embedding_journal = Embedding(input_dim=len(journal_to_int),\n",
    "                        output_dim=JOURNAL_DIM,\n",
    "                        input_length=1,\n",
    "                        trainable=TRAIN_EMBEDDING,\n",
    "                        )\n",
    "\n",
    "embedding_abstract = Embedding(input_dim=len(word_to_int) + 1,\n",
    "                        output_dim=W2V_VEC_DIM,\n",
    "                        weights=[embeddings],\n",
    "                        input_length=LEN_ABSTRACT,\n",
    "                        trainable=TRAIN_EMBEDDING,\n",
    "                        )\n",
    "\n",
    "embedding_title = Embedding(input_dim=len(word_to_int) + 1,\n",
    "                        output_dim=W2V_VEC_DIM,\n",
    "                        weights=[embeddings],\n",
    "                        input_length=LEN_TITLE,\n",
    "                        trainable=TRAIN_EMBEDDING,\n",
    "                        )\n",
    "\n",
    "journal_1_output = Flatten()(embedding_journal(journal_1))\n",
    "journal_2_output = Flatten()(embedding_journal(journal_2))\n",
    "\n",
    "authors_1_output = LSTM(AUTHORS_DIM)(embedding_authors(authors_1))\n",
    "authors_2_output = LSTM(AUTHORS_DIM)(embedding_authors(authors_2))\n",
    "\n",
    "title_1_cnn_1 = Conv1D(T_FEATURES, 3,\n",
    "               padding='same',\n",
    "               strides=1,\n",
    "               activation='elu',\n",
    "               input_shape=(LEN_TITLE, W2V_VEC_DIM)\n",
    "               )\n",
    "title_1_cnn_2 = Conv1D(T_FEATURES, 3,\n",
    "               strides=1,\n",
    "               padding='same',\n",
    "               activation='elu',\n",
    "               input_shape=(LEN_TITLE, W2V_VEC_DIM)\n",
    "               )\n",
    "\n",
    "title_2_cnn_1 = Conv1D(T_FEATURES, 3,\n",
    "               padding='same',\n",
    "               strides=1,\n",
    "               activation='elu',\n",
    "               input_shape=(LEN_TITLE, W2V_VEC_DIM)\n",
    "               )\n",
    "title_2_cnn_2 = Conv1D(T_FEATURES, 3,\n",
    "               strides=1,\n",
    "               padding='same',\n",
    "               activation='elu',\n",
    "               input_shape=(LEN_TITLE, W2V_VEC_DIM)\n",
    "               )\n",
    "\n",
    "abstract_1_cnn_1 = Conv1D(A_FEATURES, 7,\n",
    "                   strides=1,  # 1\n",
    "                   padding='same',\n",
    "                   activation='elu',\n",
    "                   input_shape=(LEN_ABSTRACT, W2V_VEC_DIM)\n",
    "                   )\n",
    "abstract_1_cnn_2 = Conv1D(A_FEATURES, 7,\n",
    "                   strides=1,  # 1\n",
    "                   padding='same',\n",
    "                   activation='elu',\n",
    "                   input_shape=(LEN_ABSTRACT, W2V_VEC_DIM)\n",
    "                   )\n",
    "\n",
    "abstract_2_cnn_1 = Conv1D(A_FEATURES, 5,\n",
    "                   strides=1,  # 1\n",
    "                   padding='same',\n",
    "                   activation='elu',\n",
    "                   input_shape=(LEN_ABSTRACT, W2V_VEC_DIM)\n",
    "                   )\n",
    "abstract_2_cnn_2 = Conv1D(A_FEATURES, 5,\n",
    "                   strides=1,  # 1\n",
    "                   padding='same',\n",
    "                   activation='elu',\n",
    "                   input_shape=(LEN_ABSTRACT, W2V_VEC_DIM)\n",
    "                   )\n",
    "\n",
    "title_1_output = GlobalMaxPool1D()(\n",
    "    Dropout(DROPOUT)(\n",
    "        title_1_cnn_2(\n",
    "            MaxPooling1D()(\n",
    "                Dropout(DROPOUT)(\n",
    "                    title_1_cnn_1(\n",
    "                        embedding_title(title_1)\n",
    "))))))\n",
    "                                   \n",
    "title_2_output = GlobalMaxPool1D()(\n",
    "    Dropout(DROPOUT)(\n",
    "        title_2_cnn_2(\n",
    "            MaxPooling1D()(\n",
    "                Dropout(DROPOUT)(\n",
    "                    title_2_cnn_1(\n",
    "                        embedding_title(title_2)\n",
    "))))))\n",
    "                                   \n",
    "abstract_1_output = GlobalMaxPool1D()(\n",
    "    Dropout(DROPOUT)(\n",
    "        abstract_1_cnn_2(\n",
    "            MaxPooling1D()(\n",
    "                Dropout(DROPOUT)(\n",
    "                    abstract_1_cnn_1(\n",
    "                        embedding_abstract(abstract_1)\n",
    "))))))\n",
    "                                      \n",
    "abstract_2_output = GlobalMaxPool1D()(\n",
    "    Dropout(DROPOUT)(\n",
    "        abstract_2_cnn_2(\n",
    "            MaxPooling1D()(\n",
    "                Dropout(DROPOUT)(\n",
    "                    abstract_2_cnn_1(\n",
    "                        embedding_abstract(abstract_2)\n",
    "))))))\n",
    "\n",
    "dense_input = concatenate_layers([authors_1_output,\n",
    "                                  authors_2_output,\n",
    "                                  journal_1_output,\n",
    "                                  journal_2_output,\n",
    "                                  title_1_output, \n",
    "                                  title_2_output, \n",
    "                                  abstract_1_output, \n",
    "                                  abstract_2_output, \n",
    "                                  other_features])\n",
    "\n",
    "dense_1 = Dropout(.40)(Dense(256, activation='elu')(dense_input))\n",
    "dense_2 = Dropout(.40)(Dense(128, activation='elu')(dense_1))\n",
    "output = Dense(1, activation='sigmoid', name=\"output\")(dense_2)\n",
    "\n",
    "cnn_input = concatenate_layers([title_1_output, \n",
    "                                  title_2_output, \n",
    "                                  abstract_1_output, \n",
    "                                  abstract_2_output])\n",
    "\n",
    "cnn_1 = Dense(128, activation='elu')(cnn_input)\n",
    "cnn_output = Dense(1, activation='sigmoid', name='cnn_output')(cnn_1)\n",
    "\n",
    "\n",
    "model = Model(inputs=[authors_1, \n",
    "                      authors_2, \n",
    "                      journal_1, \n",
    "                      journal_2, \n",
    "                      title_1, \n",
    "                      title_2, \n",
    "                      abstract_1, \n",
    "                      abstract_2, \n",
    "                      other_features], outputs=(output, cnn_output))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "display(SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 20\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_output_acc',\n",
    "                               patience=3,\n",
    "                               mode='max',\n",
    "                               verbose=1)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"cnn-{val_output_acc:.4f}.hdf5\",\n",
    "                               monitor='val_output_acc',\n",
    "                               save_best_only=True,\n",
    "                               verbose=1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_output_acc',\n",
    "                              factor=0.33,\n",
    "                              patience=1,\n",
    "                              cooldown=1,\n",
    "                              verbose=True)\n",
    "\n",
    "\n",
    "model.fit_generator(\n",
    "    cnn_batchify(training_indexes, training_set, training_set_output, BATCH_SIZE),\n",
    "    epochs=N_EPOCHS,\n",
    "    steps_per_epoch=int(len(training_set)/BATCH_SIZE)+1,\n",
    "    validation_data=cnn_batchify(testing_indexes, testing_set, testing_set_output, BATCH_SIZE),\n",
    "    validation_steps=int(len(testing_set)/BATCH_SIZE)+1,\n",
    "    callbacks=[early_stopping, checkpointer, reduce_lr],\n",
    "    use_multiprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputify_gen(source: List[List], out, batch_size=512):\n",
    "    while True:\n",
    "        o_s = 0\n",
    "        features = np.empty((batch_size, len(\n",
    "            list(entry(nodes_dict[source[0][0]], nodes_dict[source[0][0]])))))\n",
    "        for i, el in enumerate(source):\n",
    "            node_a = nodes_dict[el[0]]\n",
    "            node_b = nodes_dict[el[1]]\n",
    "            features[i % batch_size] = entry(node_a, node_b)\n",
    "            if i % batch_size == 0 and i != 0:\n",
    "                yield normaliser.transform(features), out[o_s:i]\n",
    "                o_s = i\n",
    "        yield normaliser.transform(features[:i % batch_size]), out[o_s:i]\n",
    "\n",
    "\n",
    "normaliser = StandardScaler()\n",
    "sample = list(inputify(training_input[:10000]))\n",
    "normaliser.fit(sample)\n",
    "i = int(len(training_input) * .9)\n",
    "\n",
    "\n",
    "\n",
    "model.fit_generator(inputify_gen(training_input[:i], training_output[:i], 512),\n",
    "                    steps_per_epoch=int(i/512),\n",
    "                    validation_data=inputify_gen(\n",
    "                        training_input[i:], training_output[i:], 512),\n",
    "                    validation_steps=int((len(training_input)-i)/512),\n",
    "                    epochs=N_EPOCHS,\n",
    "                    callbacks=[early_stopping, checkpointer, reduce_lr],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "03cbfbd0300c2545e1df12cb2bcd8ae84cd4379a"
   },
   "outputs": [],
   "source": [
    "gen = content_generator(\n",
    "    training_input[len(train_test_i):],\n",
    "    test_test_i,\n",
    "    test_test_o\n",
    ")\n",
    "\n",
    "inp = {'titles_1': np.empty((len(test_test_i), LEN_TITLE)),\n",
    "       'titles_2': np.empty((len(test_test_i), LEN_TITLE)),\n",
    "       'abstracts_1': np.empty((len(test_test_i), LEN_ABSTRACT)),\n",
    "       'abstracts_2': np.empty((len(test_test_i), LEN_ABSTRACT)),\n",
    "       'other_features': np.empty((len(test_test_i), len(train_test_i[0]))),\n",
    "       }\n",
    "\n",
    "for i, (_, el) in enumerate(zip(test_test_i, gen)):\n",
    "    inp['titles_1'][i] = el[0]\n",
    "    inp['titles_2'][i] = el[1]\n",
    "    inp['abstracts_1'][i] = el[2]\n",
    "    inp['abstracts_2'][i] = el[3]\n",
    "    inp['other_features'][i] = el[5]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predictions = model.predict(inp)\n",
    "\n",
    "threshold = thresholdify(predictions[0], test_test_o)\n",
    "f1 = mean_f1(predictions[0], test_test_o, threshold)\n",
    "print(\"Mean F1 : %f\\nThreshold : %f\" % (f1, threshold))\n",
    "\n",
    "threshold = thresholdify(predictions[1], test_test_o)\n",
    "f1 = mean_f1(predictions[1], test_test_o, threshold)\n",
    "print(\"Mean F1 : %f\\nThreshold : %f\" % (f1, threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "12ffba57bb334aa06a12a06461d81c4e15844bbc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testing_set = inputify(testing_input)\n",
    "testing_set = normaliser.transform(testing_set)\n",
    "\n",
    "inp = {'titles_1': np.empty((len(testing_set), LEN_TITLE)),\n",
    "       'titles_2': np.empty((len(testing_set), LEN_TITLE)),\n",
    "       'abstracts_1': np.empty((len(testing_set), LEN_ABSTRACT)),\n",
    "       'abstracts_2': np.empty((len(testing_set), LEN_ABSTRACT)),\n",
    "       'other_features': np.empty((len(testing_set), len(train_test_i[0]))),\n",
    "       }\n",
    "\n",
    "gen = content_generator(\n",
    "    testing_input,\n",
    "    testing_set,\n",
    "    list(range(len(test_test_o)))\n",
    ")\n",
    "\n",
    "for i, (_, el) in enumerate(zip(testing_set, gen)):\n",
    "    inp['titles_1'][i] = el[0]\n",
    "    inp['titles_2'][i] = el[1]\n",
    "    inp['abstracts_1'][i] = el[2]\n",
    "    inp['abstracts_2'][i] = el[3]\n",
    "    inp['other_features'][i] = el[5]\n",
    "\n",
    "testing_output = [int(el + .5) for el in model.predict(inp)[0]]\n",
    "\n",
    "with open(\"out.csv\", \"w+\") as f:\n",
    "    f.write(\"id,category\\n\")\n",
    "    for i, v in enumerate(testing_output):\n",
    "        f.write(\"%d,%d\\n\" % (i, v))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "279px",
    "width": "236px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
